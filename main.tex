\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage[vlined,ruled]{algorithm2e}

\DeclareMathOperator*{\argmin}{\operatorname*{arg\,min}} % Jan Hlavacek
\DeclareMathOperator*{\argmax}{\operatorname*{arg\,max}} % Jan Hlavacek

%% Sets page size and margins
\usepackage[a4paper,top=1cm,bottom=1cm,left=2cm,right=1cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\newcommand\mycommfont[1]{\footnotesize\ttfamily\textcolor{gray}{#1}}
\SetCommentSty{mycommfont}

\title{Using SOMs for the detection of sensorimotor contingencies.}
\author{}

\begin{document}
\maketitle

%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------

\section*{Self-Organizing Maps}

\subsection*{Self-Organizing Map - (SOM)}

The SOM is a neural network that learns to map the n-dimensional space of the inputs to an arbitrary k-dimensional space in an unsupervised manner. 
The idea behind SOMs is very similar to k.-mean clustering. As for k-mean clustering each output unit is a filter of the input that responds maximally to those input patterns that are closer to a particular vector of weights. This weight vector is the prototype of the related cluster. Differently from k-mean clustering the prototypes of a SOM have a further constraint: the must keep the euclidean distance from their neighborhood (prototypes that are nearby in the output layer) as small as possible.
Thus SOM Optimization is based on a specialization of the k-means cost function. Only those weights prototypes that are currently the nearest to an input pattern (and their neighborhood) are recruited for optimization as in k-mean clustering,  but here recruitment is not a binary function. The update of each weight vector is modulated by the radial basis of the distance of the related output unit from the output unit which is the closest to the current input (the winner unit).   

\IncMargin{1em}
\begin{algorithm}
\SetAlgorithmName{Code}{Code}{Code}
\DontPrintSemicolon
\SetKwInOut{Parameters}{Parameters}
\KwData{$\mathbf{X}\in \mathbb{R}^{N\times M}$, the batch of input patterns}   

\Parameters{$M$, Input patterns length
\\$N$ Number of samples
\\$K$ Number of outputs
\\$\eta_0$ Initial learning rate, $\tau_{\eta}$ learning rate decay
\\$\sigma_0$ Initial Neighborhood, $\tau_{\sigma}$ neighborhood decay
}
\BlankLine
\Begin{
    $\mathbf{W} \leftarrow \mathbf{0}\in\mathbb{R}^{K\times M}$
	\BlankLine
    \For{$t\leftarrow 1$ \KwTo {$T$}}{
     	$\eta_t = e^{-\frac{t}{T\tau_{\eta}}}$
     	$\sigma_t = e^{-\frac{t}{T\tau_{\sigma}}}$
     	\tcp{Compute the current modulation of the prototypes as a function of the distance between prototypes and inputs}
        \For{$n\leftarrow 1$ \KwTo {$N$}}{
		    \BlankLine
            \tcp{Winning prototype}
    	    $r_{n}\leftarrow \argmin_k || 
            \mathbf{x}_n - \mathbf{w}_k ||$
		    \BlankLine
            \tcp{Radial bases of distances from winner}
            \For{$k\leftarrow 1$ \KwTo {$K$}}{
        	    $\phi_{n, k}\leftarrow e^{-\frac{\left(k - r_{n}\right)^2}{2\sigma_t^2}}$
            }
        }
        \BlankLine
        \tcp{The cost function is the squared error of inputs from weights
        {\color{red!60} modulated by $\boldsymbol{\Phi}$}}
        $L\left(\mathbf{W}\right) = \frac{1}{2} \sum\limits_{n} \sum\limits_{k} 
        \phi_{n, k} || \mathbf{x}_n - \mathbf{w}_k 
        ||^2$
        \tcp{Gradient descent}
        $\mathbf{W} \leftarrow \mathbf{W} - \eta_t\frac{\partial L\left(\mathbf{W}\right)}{\partial\mathbf{W}} \equiv \mathbf{W} + \eta_t\sum\limits_{n} \sum\limits_{k} \phi_{n, k}(\mathbf{x}_{n}-\mathbf{w}_{k})$
    }
}
\caption{SOM training algorithm}
\label{somscript}
\end{algorithm}
\DecMargin{1em}

%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------

\subsection*{Modulated Self-Organizing Map (MSOM)}
     The idea of SOM can be further generalized by substituting the modulation based on the distance between prototypes and inputs with one acting independently Thus the topology is constrained on an external factor.


\IncMargin{1em}
\begin{algorithm}[H]
\DontPrintSemicolon
\SetAlgorithmName{Code}{Code}{Code}
\SetKwInOut{Parameters}{Parameters}
\KwData{$\mathbf{X}\in \mathbb{R}^{N\times M}$, the batch of input patterns}   

\Parameters{$M$, input pattern length
\\$N$ number of samples
\\$K$ Number of outputs
\\$\eta_0$ initial learning rate, $\tau_{\eta}$ learning rate decay
}

\BlankLine

\Begin{

    $\mathbf{W} \leftarrow \mathbf{0}\in\mathbb{R}^{M\times K}$

	\BlankLine
	
    
    \For{$t\leftarrow 1$ \KwTo {$T$}}{
    	\BlankLine
        
     	\tcp{The current modulation of the prototypes is taken from outside and is a function of the distance {\color{red!60} between prototypes and an external factor}}        \text{get}\,$\boldsymbol{\Phi} \in \mathbb{R}^{N\times K}$
    
        \BlankLine
        
        \tcp{The cost function is the squared error of inputs from weights
        {\color{red!60} modulated by $\boldsymbol{\Phi}$}}
        $L\left(\mathbf{W}\right) = \frac{1}{2} \sum\limits_{n} \sum\limits_{k} 
        \phi_{k, n} || \mathbf{x}_n - \mathbf{w}_k 
        ||^2$
        
        \tcp{Gradient descent}
        $\mathbf{W} \leftarrow \mathbf{W} - \eta_t\frac{\partial L\left(\mathbf{W}\right)}{\partial\mathbf{W}} \equiv \mathbf{W} + \eta_t\sum\limits_{n} \sum\limits_{k} \phi_{k, n}(\mathbf{x}_{n}-\mathbf{w}_{k})$
        
    }
}
\caption{MSOM training algorithm}
\label{msomscript}
\end{algorithm}
\DecMargin{1em}

%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
\pagebreak
\section*{Functions for SOMs}


\IncMargin{1em}
\begin{algorithm}
\SetAlgorithmName{Code}{Code}{Code}
\DontPrintSemicolon
\SetKwProg{Fn}{Function}{:}{}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}

\Input{$\mathbf{X}\in\mathbb{R}^{N\times M}$ Input patterns\\
$N$ number of samples\\
$K$ Number of outputs \\
$\boldsymbol{\sigma}$ Current assumed neighborhood
}
\Output{$\mathbf{O}\in\mathbb{R}^{N\times K}$ Output activation}


\SetKwFunction{SpreadSOM}{SpreadSOM}

\BlankLine
\Fn{\SpreadSOM{$\mathbf{X}$, $K$, $N$, $\boldsymbol{\sigma}$}}{
    \For{$n\leftarrow 1$ \KwTo {$N$}}{
        \BlankLine
        \tcp{Winning prototype}
    	$r_{n}\leftarrow \argmin_k || 
        \mathbf{x}_n - \mathbf{w}_k ||$
        \BlankLine
        \tcp{Radial bases of distances from winner}
        \For{$k\leftarrow 1$ \KwTo {$K$}}{
            $o_{n, k}\leftarrow e^{-\frac{\left(k - r_{n}\right)^2}{2\sigma_k^2}}$
        }
    }
    \Return{$\mathbf{O}$}
}
\caption{SOM feedforward activation}
\label{somspread}
\end{algorithm}
\DecMargin{1em}

\IncMargin{1em}
\begin{algorithm}
\SetAlgorithmName{Code}{Code}{Code}
\DontPrintSemicolon
\SetKwProg{Fn}{Function}{:}{}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{$\mathbf{X}\in\mathbb{R}^{N\times M}$ Input patterns\\
$\mathbf{W}\in\mathbb{R}^{K, M}$ weights of the SOM\\
$\boldsymbol{\Phi}\in\mathbb{R}^{K, M}$ modulations of the weights}
\Output{$\mathbf{W}\in\mathbb{R}^{K, M}$ weights of the SOM}

\SetKwFunction{UpdateSOM}{UpdateSOM}

\BlankLine
\Fn{\UpdateSOM{$\mathbf{X}$,$\mathbf{W}$, $\boldsymbol{\Phi}$, $K$, $N$}}{
    
    $\mathbf{W} \leftarrow  \mathbf{W} + \eta_t\sum\limits_{n} \sum\limits_{k} \phi_{k, n}(\mathbf{x}_{n}-\mathbf{w}_{k})$
    
    \Return{$\mathbf{W}$}
}
\caption{SOM Update weights}
\label{somupdate}
\end{algorithm}
\DecMargin{1em}

\IncMargin{1em}
\begin{algorithm}
\SetAlgorithmName{Code}{Code}{Code}
\DontPrintSemicolon
\SetKwProg{Fn}{Function}{:}{}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}

\Input{$\mathbf{O}\in\mathbb{R}^{N, K}$ activations of the ouput layer\\
$\mathbf{W}\in\mathbb{R}^{K, M}$ weights of the SOM}
\Output{$\mathbf{X_{gen}}\in\mathbb{R}^{N\times K}$ Generated input}


\SetKwFunction{BackpropagateSOM}{BackpropagateSOM}

\BlankLine

\Fn{\BackpropagateSOM{$\mathbf{O}$ ,$\mathbf{W}$, $K$, $N$}}{
    
    \tcp{Back-propagate to the input space}
   	$\boldsymbol{X}_{gen} \leftarrow \mathbf{O}\mathbf{W}$
    
    \Return{$\boldsymbol{X}_{gen}$}
}
\caption{SOM backward activation}
\label{sombkward}
\end{algorithm}
\DecMargin{1em}

\IncMargin{1em}
\begin{algorithm}
\SetAlgorithmName{Code}{Code}{Code}
\DontPrintSemicolon
\SetKwProg{Fn}{Function}{:}{}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\SetKwInOut{Parameters}{Parameters}

\Input{$\mathbf{Y}\in\mathbb{R}^{N}$ points in the output layer}
\Output{$\boldsymbol{\Psi}\in\mathbb{R}^{N\times K}$ Radial bases of the points}
\Parameters{
$N$ number of points \\ 
$K$ Number of radial bases per point \\
$\mathbf{\sigma} \in \mathbb{R}^{K}$ Extension of the radial bases} 

\SetKwFunction{RadialBases}{RadialBases}

\BlankLine

\Fn{\RadialBases{$\mathbf{y}$, $N$, $K$, $\boldsymbol{\sigma}$}}{
    
    \tcp{Encode the $y$ point as activities of the outputs}
    \For{$n\leftarrow 1$ \KwTo {$N$}}{
        \For{$k\leftarrow 1$ \KwTo {$K$}}{
        $\psi_{k, n} \leftarrow 
        	\left(\sigma_{k}\sqrt{2\pi}\right)^{-1}\exp\left(-0.5\left(
        	\left(k - y_n\right) 
        	\sigma_{k}^{-1}\right)^2\right)$            
        }
    }
    \Return{$\boldsymbol{\Psi}$}
}
\caption{Compute radial bases from point}
\label{radialBases}
\end{algorithm}
\DecMargin{1em}

\pagebreak


%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------

%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------
%----------------------------------------------------------------------------------

\pagebreak
\ 
\pagebreak

\section*{Using SOMs to find sensorimotor contingencies}

\IncMargin{1em}
\begin{algorithm}[H]
\SetAlgorithmName{Code}{Code}{Code}

\DontPrintSemicolon
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{$\boldsymbol{\xi}\in\mathbb{R}^{K}$ Probabilities of the points in the output space\\
$N$ Number of samples}    
\Output{$\mathbf{s}\in\mathbb{R}^{N}$ Generated samples}
\SetKwFunction{GetSamples}{GetSamples}
\SetKwProg{Fn}{Function}{:}{}

\Fn{\GetSamples{$\boldsymbol{\xi}$}}{
	

    $p(y_k|\xi_k) = \mathcal{N}(k, \xi_k)$


    $p(y|\boldsymbol{\xi}) = \sum_k \frac{\xi_k\, p(y_k)}
    {\sum_j\xi_j}$
    
    \For{$n=1$ \KwTo $N$}{
        $s_n \sim p(y|\boldsymbol{\xi})$
    }
    
   	\Return{$\mathbf{s}$}
}

\caption{Sampling points from a mixed probability distribution}
\label{samples}
\end{algorithm}
\DecMargin{1em}


\IncMargin{1em}
\begin{algorithm}[H]
\SetAlgorithmName{Code}{Code}{Code}

\DontPrintSemicolon
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{$\mathbf{X}\in\mathbb{R}^{N\times K}$ Encoded patterns, \\ 
	$\mathbf{w}\in\mathbb{R}^{K}$ Prediction weights,\\
	$\alpha$ Prediction sensitivity}    
\Output{$\mathbf{y}\in\mathbb{R}^{N}$}
\SetKwFunction{Predict}{Predict}
\SetKwProg{Fn}{Function}{:}{}

\BlankLine

\Fn{\Predict{$\mathbf{X}$, $\mathbf{w}$, $\alpha$}}{
	
    \BlankLine
    
	$\mathbf{y} \leftarrow \left(1 + e^{-\alpha\left(\mathbf{X}\mathbf{w} - 0.5\right)}\right)^{-1}$\;
   	
   	\Return{$\mathbf{y}$}
}
\caption{Prediction function - predicts the outcome of the inputs (from 0/unsuccessful to 1/successful)  }
\label{predspread}
\end{algorithm}
\DecMargin{1em}

\IncMargin{1em}
\begin{algorithm}[H]
\SetAlgorithmName{Code}{Code}{Code}

\DontPrintSemicolon
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}
\Input{$\mathbf{X}\in\mathbb{R}^{N\times K}$ Encoded patterns,\\
	$\boldsymbol{\lambda}\in\mathbb{R}^{N}$ Targets for each pattern (0/successful to 1/unsuccessful),\\
    $\mathbf{w}\in\mathbb{R}^{K}$ Prediction weights,\\ 
    $\eta$ Learning rate,\\ 
    $\alpha$ Prediction sensitiveness}    
\Output{$\mathbf{w}\in\mathbb{R}^{K}$}
\SetKwFunction{UpdatePredictor}{UpdatePredictor}
\SetKwProg{Fn}{Function}{:}{}
\BlankLine
\BlankLine
\Fn{\UpdatePredictor{$\mathbf{X}$, $\boldsymbol{\lambda}$, 
	$\mathbf{w}$, $\eta$, $\alpha$}}{
    
	\BlankLine
   
	$\mathbf{y }\leftarrow \Predict{$\mathbf{X}$,$\mathbf{w}$, $\alpha$}$\;
    $\mathbf{w} \leftarrow \mathbf{w} + \eta\alpha
    	\sum_{i=1}^N{\mathbf{x_i}y_i(1-y_i)
        \left(y_i -  \lambda_i\right)}$
     
	\BlankLine  

   	\Return{$\mathbf{w}$}
}
\caption{Update prediction weights based on a target}
\label{predupdate}
\end{algorithm}
\DecMargin{1em}

\subsection*{The algorithm}


\IncMargin{1em}
\begin{algorithm}[H]
\SetAlgorithmName{Code}{Code}{Code}

\DontPrintSemicolon\SetKwInOut{Encoding}{Encoding weights}    
\SetKwInOut{Motor}{Motor-control weights}    
\SetKwInOut{Pred}{Prediction weights}    
\SetKwInOut{S}{States}    
\SetKwInOut{A}{Actions}    
\SetKwInOut{G}{Goals}    
\SetKwInOut{GE}{Encoded goals}    
\SetKwInOut{SE}{Encoded states}    
\SetKwInOut{M}{Dimensions in sensory/motor space\ \ \ \ \ \ }    
\SetKwInOut{K}{Clusters in the encoding space}    
\SetKwInOut{N}{Number of samples}    
\SetKwInOut{B}{Amplitude of the output grid radial bases \ \ \ \ \ \ }    
\SetKwInOut{AA}{Temperature of the prediction function\ \ \ \ \ \ }    
\SetKwInOut{GG}{Temperature of the uncertainty function\ \ \ \ \ \ }    
\SetKwInOut{KK}{Amplitude of the prediction radial bases \ \ \ \ \ \ }    
\SetKwInOut{Nu}{Amplitude of the motor exploration\ \ \ \ \ \ }    
\SetKwInOut{Thm}{Threshold of the reward region\ \ \ \ \ \ }    
\SetKwInOut{Rho}{Amplitude of the match region\ \ \ \ \ \ }    

\SetKwFunction{RadialBases}{RadialBases}
\SetKwFunction{BackpropagateSOM}{BackpropagateSOM}
\SetKwFunction{SpreadSOM}{SpreadSOM}
\SetKwFunction{UpdateSOM}{UpdateSOM}
\SetKwFunction{UpdatePredictor}{UpdatePredictor}
\SetKwFunction{GetSamples}{GetSamples}
\SetKwFunction{Predict}{Predict}

\Encoding{$\mathbf{W_s}\in\mathbb{R}^{K\times M}$}
\Motor{$\mathbf{W_m}\in\mathbb{R}^{K\times M}$}
\Pred{$\mathbf{w_p}\in\mathbb{R}^{K}$}
\A{$\mathbf{a}\in\mathbb{R}^{N}$}
\S{$\mathbf{s}\in\mathbb{R}^{N}$}
\G{$\mathbf{g}\in\mathbb{R}^{N}$}
\GE{$\mathbf{G_{enc}}\in\mathbb{R}^{N\times K}$}
\SE{$\mathbf{S_{enc}}\in\mathbb{R}^{N\times K}$}
\M{$M$}
\N{$N$}
\K{$K$}
\AA{$\alpha$}
\GG{$\gamma$}
\KK{$\beta$}
\Nu{$\nu$}
\Rho{$\rho$}

\BlankLine

\Begin{

    \BlankLine
    
    \tcp{Build a grid of points in the output space 
         to record the current prediction surface}
    $\mathbf{x}\in\{1\dots K\}$\;
    $\mathbf{X_{enc}} \leftarrow \RadialBases{$\mathbf{x}$, $K$, $K$, $\beta$}$
     
     \BlankLine
       
  	 \Repeat{$\frac{1}{K}\sum_{i=1}^K\boldsymbol{\xi} = 1$}{
  	 
  	    \BlankLine
        
        \tcp{Get current prediction and uncertainty surfaces}
        $\boldsymbol{\xi}\leftarrow\Predict{$\mathbf{X_{enc}}$, 
        $\mathbf{w_p}$, $\alpha$}$\;
        $\boldsymbol{\omega}\leftarrow exp\left(-\gamma\boldsymbol{\xi}\right)$

        \BlankLine
              
        \tcp{Generate goals and actions based on current uncertainty}
        $\mathbf{g} \leftarrow \GetSamples{$\boldsymbol{\omega}$}$\;
        $\mathbf{G_{enc}} \leftarrow \RadialBases{$\mathbf{g}$, $N$, $K$, $\beta$}$\;
        $\mathbf{a} \leftarrow \BackpropagateSOM{$\mathbf{G_{enc}}$, $\mathbf{W_{m}}$}$\;
        
        $\boldsymbol{\xi_{gen}}\leftarrow\Predict{$\mathbf{G_{enc}}$, $\mathbf{w_p}$, $\alpha$}$\;
        $\boldsymbol{\omega_{gen}}\leftarrow exp\left(-\gamma\boldsymbol{\xi_{gen}}\right)$\;
        $\mathbf{a} \leftarrow \mathbf{a} + \mathcal{N}(\mathbf{0},
        	\nu\boldsymbol{\omega_{gen}})$\; 

        \BlankLine
                	
        \tcp{Produce States from actions (the simpler case - identity))}
        $\mathbf{s} \leftarrow \mathbf{a}$\;
        $\mathbf{S_{enc}} \leftarrow  \SpreadSOM{$\mathbf{s}$, $\mathbf{W_s}$, $N$, $K$, $\beta$}$
  
        \BlankLine
        
        \tcp{Compute match between goal encodings and sensory encodings}
         \For{$n=1$ \KwTo $N$}{
        	$\chi_n\leftarrow||\mathbf{g_{enc}}_n - \mathbf{s_{enc}}_n||$
        }
		$\mathbf{m} \leftarrow exp(-0.5(\boldsymbol{\chi}\rho^{-1})^2)$\;
		
    
        \BlankLine
         
        \tcp{Update sensory weights}	
        $\boldsymbol{\Phi_s} \leftarrow \RadialBases{$\mathbf{s}$,$N$, $K$, $\boldsymbol{\omega}$}$\;
        $\mathbf{W_{s}} \leftarrow  \UpdateSOM{$\mathbf{s}$, $\mathbf{W_s}$, $\boldsymbol{\Phi_s}\odot((1 - \boldsymbol{\omega})\mathbf{1}^T)\odot(\mathbf{m}\mathbf{1}^T)$, $N$, $K$}$\;
        \tcp{Update motor weights}	
        $\boldsymbol{\Phi_m} \leftarrow \RadialBases{$\mathbf{a}$,$N$, $K$, $\boldsymbol{\omega}$}$\;
        $\mathbf{W_{a}} \leftarrow  \UpdateSOM{$\mathbf{a}$, $\mathbf{W_m}$, $\boldsymbol{\Phi_m}\odot((1 - \boldsymbol{\omega})\mathbf{1}^T)\odot(\mathbf{m}\mathbf{1}^T)$, $N$, $K$}$\;
        \tcp{Update predictor}	
        $\mathbf{w_p}\leftarrow\UpdatePredictor{$\mathbf{G_{gen}}$, 
        	$\mathbf{m}$, $\mathbf{w}$, $\eta$, $\alpha$}$\;
  	
  	 }
       
}
\caption{The SOMSMC algorithm}
\label{smc3}
\end{algorithm}
\DecMargin{1em}

\end{document}
